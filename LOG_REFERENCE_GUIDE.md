# Log Reference Guide

## Understanding Your App's Output

When you run your app, the console will show logs that help understand what's happening with the AI features.

## âœ… Expected Logs After Fix

### Scenario 1: Native Library NOT Available (Current)

When you try to generate a summary from a PDF:

```
I/flutter (12853): ğŸ”„ Starting offline generation for: iesc108.pdf
I/flutter (12853): ğŸ“„ Extracting text from: /data/user/0/com.example.claudetest/app_flutter/personalFiles/iesc108.pdf
I/flutter (12853): ğŸ” Reading PDF text...
I/flutter (12853): âœ… Successfully extracted 33654 characters
I/flutter (12853): ğŸ§¹ Cleaning text...
I/flutter (12853): ğŸ“š Subject Analysis: Physics (Scores: {...})
I/flutter (12853): ğŸ“š Detected Subject: Physics
I/flutter (12853): ğŸ“Š Estimated Text Complexity: Grade 7.5
I/flutter (12853): ğŸ¤– [SummaryQuiz] Using local LLM model
I/flutter (12853): ğŸ¤– [LLM] Using model for summary generation: /data/user/0/com.example.claudetest/app_flutter/summary.gguf
I/flutter (12853): ğŸŒ [LLM] Language: en
I/flutter (12853): â„¹ï¸ [LLM] Android detected - using system libllama.so
I/flutter (12853): ğŸ”„ [LLM] Initializing Llama with model: /data/user/0/com.example.claudetest/app_flutter/summary.gguf
I/flutter (12853): âŒ [LLM] Native library error: UnsupportedError: Failed to load dynamic library 'libllama.so'
I/flutter (12853): âš ï¸ [SummaryQuiz] Native library unavailable, falling back to rule-based generation
I/flutter (12853): ğŸ“ [SummaryQuiz] Using rule-based generation (Grade 5)
I/flutter (12853): ğŸ“Š Generating mind map...
I/flutter (12853): âœ… Summary, Quiz, and Mind Map saved
```

**What's happening:**
1. âœ… PDF text extracted successfully (33654 characters)
2. âœ… Subject detected (Physics)
3. âš ï¸ Tried to use AI model but libllama.so not found
4. âœ… **Automatically fell back to rule-based generation**
5. âœ… **Summary generated successfully despite error!**
6. UI shows: Orange warning â†’ "Local AI mode requires app rebuild. Using standard generation."

### Scenario 2: Native Library IS Available (After Fix)

After implementing Solution 1 or 2 from `NATIVE_LIBRARY_FIX.md`:

```
I/flutter (12853): ğŸ”„ Starting offline generation for: iesc108.pdf
I/flutter (12853): ğŸ“„ Extracting text from: /data/user/0/com.example.claudetest/app_flutter/personalFiles/iesc108.pdf
I/flutter (12853): âœ… Successfully extracted 33654 characters
I/flutter (12853): ğŸ“š Subject Analysis: Physics (Scores: {...})
I/flutter (12853): ğŸ“š Detected Subject: Physics
I/flutter (12853): ğŸ¤– [SummaryQuiz] Using local LLM model
I/flutter (12853): ğŸ¤– [LLM] Using model for summary generation: /data/user/0/.../summary.gguf
I/flutter (12853): ğŸŒ [LLM] Language: en
I/flutter (12853): â„¹ï¸ [LLM] Android detected - using system libllama.so
I/flutter (12853): ğŸ”„ [LLM] Initializing Llama with model: /data/user/0/.../summary.gguf
I/flutter (12853): âœ… [LLM] Llama initialized successfully
I/flutter (12853): âœ¨ [LLM] Generation successful - tokens: 245
I/flutter (12853): ğŸ“Š Generating mind map...
I/flutter (12853): âœ… Summary, Quiz, and Mind Map saved
I/flutter (12853): âœ… Generated with Local AI Model!
```

**What's happening:**
1. âœ… PDF text extracted
2. âœ… Llama initialized successfully
3. âœ… AI model generated summary (245 tokens)
4. âœ… Quiz generated
5. UI shows: Green checkmark â†’ "Generated with Local AI Model!"

## ğŸ” Log Key Indicators

### Good Signs (âœ…)
```
âœ… [LLM] Llama initialized successfully
âœ¨ [LLM] Generation successful
âœ… Summary, Quiz, and Mind Map saved
âœ… Generated with Local AI Model!
```
â†’ Everything working perfectly

### Warning Signs (âš ï¸)
```
âš ï¸ [LLM] Inference failed
âš ï¸ [SummaryQuiz] Native library unavailable
âš ï¸ [SummaryQuiz] Using rule-based generation
```
â†’ Features degraded but app working

### Error Signs (âŒ) - Before Fix
```
âŒ [LLM] Error: Exception: LLM Inference Failed: LlamaException
âŒ Offline generation failed: Exception
```
â†’ App would crash (FIXED NOW âœ…)

### Error Signs (âŒ) - After Fix
```
âŒ [LLM] Native library error
```
â†’ Shows error but **app continues** with fallback âœ…

## ğŸ¯ Common Log Patterns

### Pattern 1: Missing Model File
```
âš ï¸ [LLM] Inference failed: Exception: Model not found. Please download it first.
```
**Fix:** Download models in app settings

### Pattern 2: Native Library Issues (Current Situation)
```
âŒ [LLM] Native library error: UnsupportedError: Failed to load dynamic library 'libllama.so': dlopen failed: library "libllama.so" not found
```
**Status:** âœ… HANDLED - Falls back to rule-based generation
**Fix:** Follow Solution 1 or 2 in `NATIVE_LIBRARY_FIX.md`

### Pattern 3: Empty Response
```
âš ï¸ [LLM] Inference failed: Exception: LLM generated empty response
```
**Likely Cause:** Model timeout or corrupted file
**Fix:** Re-download model or restart app

### Pattern 4: Invalid JSON from Quiz
```
âš ï¸ [LLM] Quiz inference failed: Exception: Invalid JSON from LLM Quiz
```
**Cause:** Model returned malformed JSON
**Status:** Falls back to rule-based quiz generation

## ğŸ“± Where to Find Logs

### Android Device (Connected via ADB)
```bash
# Real-time logs
adb logcat | grep -i flutter

# All logs including errors
adb logcat | grep -E "(E/|W/|I/flutter)"

# Save to file
adb logcat > logfile.txt
```

### Android Emulator
Same commands as above, or use Android Studio's Logcat panel

### iOS
Use Xcode console or `flutter run -v` for verbose output

### Windows/macOS
Console output appears directly in the terminal

## ğŸš¨ If You See CRASH (Before Fix Applied)

If logs show:
```
E/flutter (12853): [ERROR:flutter/runtime/dart_vm_initializer.cc:41] Unhandled exception:
E/flutter (12853): LlamaException: Failed to initialize Llama...
```

This means the old code was being used. **Update to the latest version** (the fix should prevent this).

## âœ¨ Performance Indicators

### Token Generation Speed
```
âœ¨ [LLM] Generation successful - tokens: 245
```
- 245 tokens â‰ˆ ~10-15 seconds on mid-range Android device
- 500+ tokens â‰ˆ ~30-40 seconds
- Slower than server API but completely offline

### Memory Usage
Not directly shown in logs, but if you see:
```
D/Llama-Native: Initializing with 2GB context
```
â†’ App allocated ~2GB RAM for model
â†’ May cause slowdown on low-RAM devices

## ğŸ”§ Debug Commands

### Enable Verbose Logging
```bash
flutter run -v
```
Shows all internal Flutter operations plus your logs

### Check Device Logs Real-time
```bash
flutter logs
```
Filters only your app's output

### Full Device Status
```bash
flutter doctor -v
```
Shows all tools and dependencies

## ğŸ“Š Log Timeline Example

Here's a typical good run:

```
[0ms] ğŸ”„ Starting offline generation for: Math_Chapter_3.pdf
[100ms] ğŸ“„ Extracting text from: /data/user/0/.../Math_Chapter_3.pdf
[200ms] ğŸ” Reading PDF text...
[800ms] âœ… Successfully extracted 15000 characters
[850ms] ğŸ§¹ Cleaning text...
[900ms] ğŸ“š Subject Analysis: Mathematics (Scores: {...})
[950ms] ğŸ“š Detected Subject: Mathematics
[1000ms] ğŸ¤– [SummaryQuiz] Using local LLM model
[1100ms] ğŸ¤– [LLM] Using model for summary generation: ...
[1150ms] âœ… [LLM] Llama initialized successfully
[3000ms] âœ¨ [LLM] Generation successful - tokens: 200
[3100ms] ğŸ“Š Generating mind map...
[3300ms] âœ… Summary, Quiz, and Mind Map saved
[3350ms] âœ… Generated with Local AI Model!
```

Total time: ~3.3 seconds for complete offline processing

---

**Last Updated:** December 24, 2025
**Status:** Updated with fix implementation details
